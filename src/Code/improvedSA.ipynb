{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Improved sentiment analysis classifer ###\n",
    "# Uses k-fold cross validation and Naive Bayes, Decision Tree, and Bernoulli ML models #\n",
    "# Outputs average accuracy of the model #"
   ]
  },
  {
   "source": [
    "import tarfile\n",
    "import nltk\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import classify\n",
    "from nltk.classify import SklearnClassifier\n",
    "from nltk import NaiveBayesClassifier, DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.metrics.scores import precision, recall\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 65,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training/testing data files\n",
    "polaritytar = tarfile.open(\"../Data/review_polarity.tar.gz\", \"r\")\n",
    "polaritytar.extractall('../Data/Polarity_Data')\n",
    "\n",
    "nrctar = tarfile.open(\"../Data/NRC-Sentiment-Emotion-Lexicons.tar.gz\")\n",
    "nrctar.extractall('../Data/NRC_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### get all the lines from all the reviews ###\n",
    "\n",
    "# lines from negative reviews\n",
    "neglines = []\n",
    "for nfilename in os.listdir('../Data/Polarity_Data/txt_sentoken/neg'):\n",
    "    open_file = open(('../Data/Polarity_Data/txt_sentoken/neg/' + nfilename),\"r\")\n",
    "    neglines = open_file.readlines()\n",
    "\n",
    "# lines from positive reviews\n",
    "poslines = []\n",
    "for pfilename in os.listdir('../Data/Polarity_Data/txt_sentoken/pos'):\n",
    "    open_file = open(('../Data/Polarity_Data/txt_sentoken/pos/' + pfilename),\"r\")\n",
    "    poslines = open_file.readlines()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensity_file = open('../Data/NRC_Data/NRC-Sentiment-Emotion-Lexicons/NRC-Sentiment-Emotion-Lexicons/NRC-Affect-Intensity-Lexicon/NRC-AffectIntensity-Lexicon.txt')\n",
    "intensity_lines = intensity_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emotions = dict()\n",
    "# use the data from affect-intensity file\n",
    "for line in intensity_lines[1:]:\n",
    "    features = line.strip().split(\"\\t\")\n",
    "    # features[0]: the word\n",
    "    # features[2]: the primary sentiment (fear, sadness, anger, joy)\n",
    "    word_emotion = (features[0], features[2])\n",
    "    word_emotions.update({word_emotion})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenize each line based on whitespace ###\n",
    "\n",
    "# tokens for positive reviews\n",
    "poslines_tokens = []\n",
    "for line in poslines:\n",
    "    l = []\n",
    "    for word in line.split():\n",
    "        l.append(word)\n",
    "    poslines_tokens.append(l)\n",
    "# tokens for negative reviews\n",
    "neglines_tokens = []\n",
    "for line in neglines:\n",
    "    l = []\n",
    "    for word in line.split():\n",
    "        l.append(word)\n",
    "    neglines_tokens.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper function to remove non-alphanumeric characters and lowercase each token ###\n",
    "def clean_tokens(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for (token, tag) in pos_tag(tweet_tokens):\n",
    "        # removing stop words from the vocabulary also decreases performance pretty significantly\n",
    "        if len(token) != 0 and token not in string.punctuation: # and token.lower() not in stopwords.words('english'):\n",
    "            cleaned_tokens.append(token.lower())\n",
    "\n",
    "    # I also tried messing around with POS tags - appending them to the word, replacing the word, etc\n",
    "    # I think POS tags might have been more helpful if it had a label for neg words - this might have been useful for the \"conflicting sentiments\" part a few cells down\n",
    "\n",
    "    # return pos_tag(cleaned_tokens)\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean up the tokens list ###\n",
    "positive_cleaned_tokens = []\n",
    "negative_cleaned_tokens = []\n",
    "\n",
    "for tokens in poslines_tokens:\n",
    "    positive_cleaned_tokens.append(clean_tokens(tokens))\n",
    "\n",
    "for tokens in neglines_tokens:\n",
    "    negative_cleaned_tokens.append(clean_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### helper function to create the model from the tokens list ###\n",
    "def create_model(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        # True is just a placeholder value\n",
    "        yield dict([token, True] for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "positive_tokens_for_model = create_model(positive_cleaned_tokens)\n",
    "negative_tokens_for_model = create_model(negative_cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# categorize the tokens in each tweet to creat the dataset\n",
    "positive_dataset = [(t,\"Positive\")\n",
    "                     for t in positive_tokens_for_model]\n",
    "negative_dataset = [(t,\"Negative\")\n",
    "                     for t in negative_tokens_for_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we remove words from \"conflicting\" sentiments from the reviews\n",
    "# i.e. if there is a word in a review marked as positive that has a \"sadness\" label, that word will be removed\n",
    "\n",
    "# first we remove negative words from the set of positive reviews\n",
    "pos_to_remove = list()\n",
    "for (review, sentiment) in positive_dataset:\n",
    "    for word in review:\n",
    "        if word in word_emotions:\n",
    "            if word_emotions[word] == \"sadness\" or word_emotions[word] == \"anger\" or  word_emotions[word] == \"fear\":\n",
    "                pos_to_remove.append(word)\n",
    "\n",
    "# kinda convoluted way of doing it because modify review while iterating through it = bad\n",
    "for (review, sentiment) in positive_dataset:\n",
    "    for neg_word in pos_to_remove:\n",
    "        if neg_word in review.keys():\n",
    "            review.pop(neg_word)\n",
    "\n",
    "# remove positive words from negative reviews\n",
    "# interestingly enough, doing this actually decreases the performance. I'm guessing it's because the \"joy\" label is the only one that is \"positive\", and it seems like there is less \"precision\" about which words can be labelled as joyful - is \"custom\" really a \"joy\" word? or \"mucis\"? what is mucis anyways??\n",
    "# also, I'd guess that it's more common to negate a positive word to make a negative phrase than to negate a negative word and make a positive one\n",
    "# I commented it out to maximize performance, but if I had time in abundance I might try playing with scope of negation stuff to see if I could get it to improve performance\n",
    "\n",
    "# neg_to_remove = list()\n",
    "# for (review, sentiment) in negative_dataset:\n",
    "#     for word in review:\n",
    "#         if word in word_emotions:\n",
    "#             if word_emotions[word] == \"joy\":\n",
    "#                 neg_to_remove.append(word)\n",
    "\n",
    "# for (review, sentiment) in negative_dataset:\n",
    "#     for pos_word in neg_to_remove:\n",
    "#         if pos_word in review.keys():\n",
    "#             review.pop(pos_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = positive_dataset + negative_dataset\n",
    "# I commented out the line below because I wanted to see how adding or changing just one feature would improve the performance, without the random variations caused by shuffling\n",
    "#random.shuffle(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_dataset = np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Naive Bayes accuracy is: 0.8644781144781145\nDecision Tree accuracy is: 0.7104377104377105\nBernoulli accuracy is: 0.8257575757575758\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# use k-fold cross validation with k = 9 to train and test\n",
    "kfold = KFold(n_splits=9, shuffle=True, random_state=1)\n",
    "nb_mean_accuracy, dt_mean_accuracy, bern_mean_accuracy = list(), list(), list()\n",
    "\n",
    "for train, test in kfold.split(np_dataset):\n",
    "    # naive bayes classifier\n",
    "    nb_classifier = NaiveBayesClassifier.train(np_dataset[train])\n",
    "    nb_mean_accuracy.append(classify.accuracy(nb_classifier, np_dataset[test]))\n",
    "\n",
    "    # decitions tree classifier\n",
    "    dt_classifier = DecisionTreeClassifier.train(np_dataset[train])\n",
    "    dt_mean_accuracy.append(classify.accuracy(dt_classifier, np_dataset[test]))\n",
    "\n",
    "    # bernoulli classifier\n",
    "    bern_classifier = SklearnClassifier(BernoulliNB()).train(np_dataset[train])\n",
    "    bern_mean_accuracy.append(classify.accuracy(bern_classifier, np_dataset[test]))\n",
    "    \n",
    "# print the mean accuracy across all the folds for each classifier\n",
    "print(\"Naive Bayes accuracy is:\", np.mean(nb_mean_accuracy))\n",
    "print(\"Decision Tree accuracy is:\", np.mean(dt_mean_accuracy))\n",
    "print(\"Bernoulli accuracy is:\", np.mean(bern_mean_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without shuffle\n",
    "# Naive Bayes accuracy is: 0.8644781144781145\n",
    "# Decision Tree accuracy is: 0.7491582491582492\n",
    "# Bernoulli accuracy is: 0.8257575757575758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of changes (i didn't keep all of them)\n",
    "    # 1. in text_process: only append token if len > 0 and not punctuation and not stop word\n",
    "    # 2. Change n_splits to 9 (maybe 10 was too many)\n",
    "    # 3. add POS tags to each token so it's (Token, POS)\n",
    "    # 4. add in Decision Tree and Bernoulli classifiers\n",
    "    # 5. remove words with conflicting sentiment from positive"
   ]
  }
 ]
}